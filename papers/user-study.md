# User study

### [Usability evaluation considered harmful (some of the time)](https://dl.acm.org/doi/10.1145/1357054.1357074)

Why harmful some time？

- Usability ≠ Usefulness
- Test to early might destory the ideas
- Discovery vs Invention : Discovery need to test and verify, Invention not necessary works well in real world at early stage
- What to test? How to test is unclear sometimes 

Examples :

- Cross-link information -> inspired HTTP later, never built/never tested
- Sutherland's Sketchpad: not usable.

> how can we create what could become culturally significant systems if we demand that the system be validated before a culture is formed around it?


### [Evaluation Strategies for HCI Toolkit Research](https://dl.acm.org/doi/10.1145/3173574.3173610)


Evaluating:

- What to evaluate?
- What methods are appropriate?

Ways to Evaluate:

- Demonstration
- Usage
- Technical benchmark
- Heuristic

More Detail, for example:

- A/B test
- Walkthrough
- Observation
- ...

> how easy it is to use the toolkit. Common measures are users’ opinions, preferences, completion time, the number of steps (e.g. lines of code), or number of mistakes. In addi- tion, given that toolkits often propose new workflows, or enable creation of new kinds of artifacts, it is important to know if it will be useful to the target audience

> This approach(Walkthrough Demonstrations) is particularly suitable when toolkit creators want to get feedback on the utility of their toolkit, as it removes the focus from using the toolkit (as one might find in a usability study) and shifts it towards the value of having the toolkit.




### [Evaluating Creativity Support Tools in HCI Research](https://dl.acm.org/doi/10.1145/3357236.3395474)


> Many CST Evaluations Focus on Usability, not Creativity


### [Towards Better User Studies in Computer Graphics and Vision](https://arxiv.org/pdf/2206.11461.pdf)


>  Quantitative behavioral data can be captured by measuring participants’ number of clicks, time- to-complete, eye gaze, etc. 